{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40446740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import shutil\n",
    "import argparse\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "# from dataclasses import dataclass\n",
    "\n",
    "DATA_PATH = \"Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e10be2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    }
   ],
   "source": [
    "# Loading documents, using a local file from Alice In Wonderland Sample text\n",
    "\n",
    "def load_documents():\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"AliceInWonderlandSample.md\")\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "documents = load_documents()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 500,\n",
    "    length_function = len,\n",
    "    add_start_index = True\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abcc7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\agila\\AppData\\Local\\Temp\\ipykernel_9844\\3979188175.py:10: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding_model = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "CHROMA_PATH = \"chroma\"\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] \n",
    "\n",
    "# Clear out the DB first\n",
    "if os.path.exists(CHROMA_PATH):\n",
    "    shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# Create a new DB from the documents\n",
    "db = Chroma.from_documents(\n",
    "    chunks, OpenAIEmbeddings(), persist_directory = CHROMA_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fc9f2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\agila\\AppData\\Local\\Temp\\ipykernel_9844\\2800758302.py:18: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
      "C:\\Users\\agila\\AppData\\Local\\Temp\\ipykernel_9844\\2800758302.py:30: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response_text = model.predict(prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Alice is sitting beside her brother on the bank.\n",
      "Sources: ['Data\\\\AliceInWonderlandSample.md']\n"
     ]
    }
   ],
   "source": [
    "CHROMA_PATH = \"chroma\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Define your query directly\n",
    "query_text = \"Who is Alice sitting beside?\"\n",
    "\n",
    "# Prepare the DB\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "# Search the DB\n",
    "results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "if len(results) == 0 or results[0][1] < 0.7:\n",
    "    print(\"Unable to find matching results.\")\n",
    "else:\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "\n",
    "    model = ChatOpenAI()\n",
    "    response_text = model.predict(prompt)\n",
    "\n",
    "    sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
